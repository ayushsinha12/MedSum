{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 8139,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018429782528566162,
      "grad_norm": 1.2773523330688477,
      "learning_rate": 4.9698980218700083e-05,
      "loss": 5.4159,
      "step": 50
    },
    {
      "epoch": 0.036859565057132324,
      "grad_norm": 0.773535430431366,
      "learning_rate": 4.939181717655732e-05,
      "loss": 3.6389,
      "step": 100
    },
    {
      "epoch": 0.055289347585698485,
      "grad_norm": 0.7981192469596863,
      "learning_rate": 4.908465413441455e-05,
      "loss": 3.4313,
      "step": 150
    },
    {
      "epoch": 0.07371913011426465,
      "grad_norm": 0.6978963017463684,
      "learning_rate": 4.8777491092271785e-05,
      "loss": 3.3298,
      "step": 200
    },
    {
      "epoch": 0.09214891264283082,
      "grad_norm": 0.7759546637535095,
      "learning_rate": 4.847032805012901e-05,
      "loss": 3.3619,
      "step": 250
    },
    {
      "epoch": 0.11057869517139697,
      "grad_norm": 0.7568705677986145,
      "learning_rate": 4.8163165007986244e-05,
      "loss": 3.2831,
      "step": 300
    },
    {
      "epoch": 0.12900847769996315,
      "grad_norm": 0.6799579858779907,
      "learning_rate": 4.7856001965843474e-05,
      "loss": 3.2067,
      "step": 350
    },
    {
      "epoch": 0.1474382602285293,
      "grad_norm": 0.5349483489990234,
      "learning_rate": 4.75488389237007e-05,
      "loss": 3.153,
      "step": 400
    },
    {
      "epoch": 0.16586804275709546,
      "grad_norm": 0.7624514102935791,
      "learning_rate": 4.724167588155793e-05,
      "loss": 3.1923,
      "step": 450
    },
    {
      "epoch": 0.18429782528566163,
      "grad_norm": 0.8336101770401001,
      "learning_rate": 4.693451283941516e-05,
      "loss": 3.1732,
      "step": 500
    },
    {
      "epoch": 0.2027276078142278,
      "grad_norm": 0.9387838244438171,
      "learning_rate": 4.66273497972724e-05,
      "loss": 3.178,
      "step": 550
    },
    {
      "epoch": 0.22115739034279394,
      "grad_norm": 0.909470796585083,
      "learning_rate": 4.632018675512962e-05,
      "loss": 3.1487,
      "step": 600
    },
    {
      "epoch": 0.2395871728713601,
      "grad_norm": 0.8461583256721497,
      "learning_rate": 4.601302371298686e-05,
      "loss": 3.1323,
      "step": 650
    },
    {
      "epoch": 0.2580169553999263,
      "grad_norm": 0.7184539437294006,
      "learning_rate": 4.5705860670844087e-05,
      "loss": 3.1017,
      "step": 700
    },
    {
      "epoch": 0.27644673792849245,
      "grad_norm": 0.7614105939865112,
      "learning_rate": 4.5398697628701316e-05,
      "loss": 3.2316,
      "step": 750
    },
    {
      "epoch": 0.2948765204570586,
      "grad_norm": 0.6786227822303772,
      "learning_rate": 4.5091534586558545e-05,
      "loss": 3.0008,
      "step": 800
    },
    {
      "epoch": 0.3133063029856248,
      "grad_norm": 0.6684582233428955,
      "learning_rate": 4.4784371544415775e-05,
      "loss": 3.1015,
      "step": 850
    },
    {
      "epoch": 0.3317360855141909,
      "grad_norm": 0.7357620596885681,
      "learning_rate": 4.447720850227301e-05,
      "loss": 2.9619,
      "step": 900
    },
    {
      "epoch": 0.3501658680427571,
      "grad_norm": 0.8081740736961365,
      "learning_rate": 4.417004546013024e-05,
      "loss": 2.9309,
      "step": 950
    },
    {
      "epoch": 0.36859565057132326,
      "grad_norm": 0.6598358750343323,
      "learning_rate": 4.386288241798747e-05,
      "loss": 3.0565,
      "step": 1000
    },
    {
      "epoch": 0.3870254330998894,
      "grad_norm": 0.7431583404541016,
      "learning_rate": 4.35557193758447e-05,
      "loss": 3.0639,
      "step": 1050
    },
    {
      "epoch": 0.4054552156284556,
      "grad_norm": 12.191261291503906,
      "learning_rate": 4.324855633370193e-05,
      "loss": 2.9345,
      "step": 1100
    },
    {
      "epoch": 0.42388499815702174,
      "grad_norm": 0.642913281917572,
      "learning_rate": 4.2941393291559165e-05,
      "loss": 2.9943,
      "step": 1150
    },
    {
      "epoch": 0.4423147806855879,
      "grad_norm": 0.6810293793678284,
      "learning_rate": 4.263423024941639e-05,
      "loss": 2.9873,
      "step": 1200
    },
    {
      "epoch": 0.4607445632141541,
      "grad_norm": 0.559029221534729,
      "learning_rate": 4.2327067207273624e-05,
      "loss": 3.0121,
      "step": 1250
    },
    {
      "epoch": 0.4791743457427202,
      "grad_norm": 0.7281472086906433,
      "learning_rate": 4.2019904165130853e-05,
      "loss": 2.9706,
      "step": 1300
    },
    {
      "epoch": 0.4976041282712864,
      "grad_norm": 0.8445867896080017,
      "learning_rate": 4.171274112298808e-05,
      "loss": 2.9015,
      "step": 1350
    },
    {
      "epoch": 0.5160339107998526,
      "grad_norm": 0.5654202699661255,
      "learning_rate": 4.140557808084531e-05,
      "loss": 2.9434,
      "step": 1400
    },
    {
      "epoch": 0.5344636933284187,
      "grad_norm": 0.6119396686553955,
      "learning_rate": 4.109841503870255e-05,
      "loss": 2.8656,
      "step": 1450
    },
    {
      "epoch": 0.5528934758569849,
      "grad_norm": 0.7492556571960449,
      "learning_rate": 4.079125199655978e-05,
      "loss": 2.8317,
      "step": 1500
    },
    {
      "epoch": 0.5713232583855511,
      "grad_norm": 0.5814321637153625,
      "learning_rate": 4.048408895441701e-05,
      "loss": 2.9019,
      "step": 1550
    },
    {
      "epoch": 0.5897530409141172,
      "grad_norm": 0.6305776238441467,
      "learning_rate": 4.017692591227424e-05,
      "loss": 2.8512,
      "step": 1600
    },
    {
      "epoch": 0.6081828234426834,
      "grad_norm": 0.6496685743331909,
      "learning_rate": 3.9869762870131466e-05,
      "loss": 2.8306,
      "step": 1650
    },
    {
      "epoch": 0.6266126059712496,
      "grad_norm": 0.5742912888526917,
      "learning_rate": 3.95625998279887e-05,
      "loss": 2.8608,
      "step": 1700
    },
    {
      "epoch": 0.6450423884998157,
      "grad_norm": 0.5767165422439575,
      "learning_rate": 3.9255436785845925e-05,
      "loss": 2.8948,
      "step": 1750
    },
    {
      "epoch": 0.6634721710283819,
      "grad_norm": 0.6612684726715088,
      "learning_rate": 3.894827374370316e-05,
      "loss": 2.8112,
      "step": 1800
    },
    {
      "epoch": 0.681901953556948,
      "grad_norm": 0.5990036725997925,
      "learning_rate": 3.864111070156039e-05,
      "loss": 2.8635,
      "step": 1850
    },
    {
      "epoch": 0.7003317360855142,
      "grad_norm": 0.9205067157745361,
      "learning_rate": 3.833394765941762e-05,
      "loss": 2.8299,
      "step": 1900
    },
    {
      "epoch": 0.7187615186140803,
      "grad_norm": 0.7069308757781982,
      "learning_rate": 3.802678461727485e-05,
      "loss": 2.8889,
      "step": 1950
    },
    {
      "epoch": 0.7371913011426465,
      "grad_norm": 0.5094358921051025,
      "learning_rate": 3.771962157513208e-05,
      "loss": 2.8024,
      "step": 2000
    },
    {
      "epoch": 0.7556210836712127,
      "grad_norm": 0.6244612336158752,
      "learning_rate": 3.7412458532989315e-05,
      "loss": 2.8496,
      "step": 2050
    },
    {
      "epoch": 0.7740508661997788,
      "grad_norm": 0.6748182773590088,
      "learning_rate": 3.7105295490846545e-05,
      "loss": 2.8246,
      "step": 2100
    },
    {
      "epoch": 0.792480648728345,
      "grad_norm": 0.7174170017242432,
      "learning_rate": 3.6798132448703774e-05,
      "loss": 2.826,
      "step": 2150
    },
    {
      "epoch": 0.8109104312569112,
      "grad_norm": 0.523895800113678,
      "learning_rate": 3.6490969406561004e-05,
      "loss": 2.7808,
      "step": 2200
    },
    {
      "epoch": 0.8293402137854773,
      "grad_norm": 1.0455775260925293,
      "learning_rate": 3.618380636441823e-05,
      "loss": 2.8807,
      "step": 2250
    },
    {
      "epoch": 0.8477699963140435,
      "grad_norm": 0.6181491613388062,
      "learning_rate": 3.587664332227547e-05,
      "loss": 2.8208,
      "step": 2300
    },
    {
      "epoch": 0.8661997788426097,
      "grad_norm": 0.5946605205535889,
      "learning_rate": 3.556948028013269e-05,
      "loss": 2.8167,
      "step": 2350
    },
    {
      "epoch": 0.8846295613711758,
      "grad_norm": 0.5858719944953918,
      "learning_rate": 3.526231723798993e-05,
      "loss": 2.8049,
      "step": 2400
    },
    {
      "epoch": 0.903059343899742,
      "grad_norm": 0.6393921971321106,
      "learning_rate": 3.495515419584716e-05,
      "loss": 2.7678,
      "step": 2450
    },
    {
      "epoch": 0.9214891264283082,
      "grad_norm": 0.503508985042572,
      "learning_rate": 3.464799115370439e-05,
      "loss": 2.8983,
      "step": 2500
    },
    {
      "epoch": 0.9399189089568744,
      "grad_norm": 0.6633691787719727,
      "learning_rate": 3.434082811156162e-05,
      "loss": 2.7179,
      "step": 2550
    },
    {
      "epoch": 0.9583486914854404,
      "grad_norm": 0.7118969559669495,
      "learning_rate": 3.403366506941885e-05,
      "loss": 2.7696,
      "step": 2600
    },
    {
      "epoch": 0.9767784740140066,
      "grad_norm": 0.5705476403236389,
      "learning_rate": 3.372650202727608e-05,
      "loss": 2.8249,
      "step": 2650
    },
    {
      "epoch": 0.9952082565425728,
      "grad_norm": 0.6078850030899048,
      "learning_rate": 3.3419338985133305e-05,
      "loss": 2.7878,
      "step": 2700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.55727481842041,
      "eval_runtime": 105.757,
      "eval_samples_per_second": 22.807,
      "eval_steps_per_second": 2.856,
      "step": 2713
    },
    {
      "epoch": 1.013638039071139,
      "grad_norm": 0.6188091039657593,
      "learning_rate": 3.311217594299054e-05,
      "loss": 2.8671,
      "step": 2750
    },
    {
      "epoch": 1.0320678215997052,
      "grad_norm": 0.7471727728843689,
      "learning_rate": 3.280501290084777e-05,
      "loss": 2.8682,
      "step": 2800
    },
    {
      "epoch": 1.0504976041282712,
      "grad_norm": 0.5536347031593323,
      "learning_rate": 3.249784985870501e-05,
      "loss": 2.799,
      "step": 2850
    },
    {
      "epoch": 1.0689273866568374,
      "grad_norm": 0.5562173128128052,
      "learning_rate": 3.219068681656223e-05,
      "loss": 2.7356,
      "step": 2900
    },
    {
      "epoch": 1.0873571691854036,
      "grad_norm": 0.6290777325630188,
      "learning_rate": 3.1883523774419466e-05,
      "loss": 2.6605,
      "step": 2950
    },
    {
      "epoch": 1.1057869517139698,
      "grad_norm": 0.6797436475753784,
      "learning_rate": 3.1576360732276695e-05,
      "loss": 2.8338,
      "step": 3000
    },
    {
      "epoch": 1.124216734242536,
      "grad_norm": 0.7407424449920654,
      "learning_rate": 3.1269197690133925e-05,
      "loss": 2.711,
      "step": 3050
    },
    {
      "epoch": 1.1426465167711022,
      "grad_norm": 0.9832471609115601,
      "learning_rate": 3.0962034647991154e-05,
      "loss": 2.8085,
      "step": 3100
    },
    {
      "epoch": 1.1610762992996682,
      "grad_norm": 0.6796048879623413,
      "learning_rate": 3.0654871605848384e-05,
      "loss": 2.7101,
      "step": 3150
    },
    {
      "epoch": 1.1795060818282344,
      "grad_norm": 0.6645605564117432,
      "learning_rate": 3.0347708563705616e-05,
      "loss": 2.7523,
      "step": 3200
    },
    {
      "epoch": 1.1979358643568006,
      "grad_norm": 0.6157718896865845,
      "learning_rate": 3.0040545521562846e-05,
      "loss": 2.7405,
      "step": 3250
    },
    {
      "epoch": 1.2163656468853667,
      "grad_norm": 1.0044106245040894,
      "learning_rate": 2.973338247942008e-05,
      "loss": 2.7451,
      "step": 3300
    },
    {
      "epoch": 1.234795429413933,
      "grad_norm": 0.5953869819641113,
      "learning_rate": 2.9426219437277308e-05,
      "loss": 2.7718,
      "step": 3350
    },
    {
      "epoch": 1.2532252119424991,
      "grad_norm": 0.6016568541526794,
      "learning_rate": 2.9119056395134538e-05,
      "loss": 2.6943,
      "step": 3400
    },
    {
      "epoch": 1.2716549944710653,
      "grad_norm": 0.5510916113853455,
      "learning_rate": 2.881189335299177e-05,
      "loss": 2.6673,
      "step": 3450
    },
    {
      "epoch": 1.2900847769996315,
      "grad_norm": 0.5760928392410278,
      "learning_rate": 2.8504730310849e-05,
      "loss": 2.75,
      "step": 3500
    },
    {
      "epoch": 1.3085145595281975,
      "grad_norm": 0.6176618337631226,
      "learning_rate": 2.8197567268706233e-05,
      "loss": 2.7359,
      "step": 3550
    },
    {
      "epoch": 1.3269443420567637,
      "grad_norm": 0.5918596982955933,
      "learning_rate": 2.789040422656346e-05,
      "loss": 2.6959,
      "step": 3600
    },
    {
      "epoch": 1.34537412458533,
      "grad_norm": 0.9635293483734131,
      "learning_rate": 2.7583241184420695e-05,
      "loss": 2.5851,
      "step": 3650
    },
    {
      "epoch": 1.363803907113896,
      "grad_norm": 0.6354343891143799,
      "learning_rate": 2.727607814227792e-05,
      "loss": 2.6662,
      "step": 3700
    },
    {
      "epoch": 1.3822336896424623,
      "grad_norm": 0.5506047606468201,
      "learning_rate": 2.696891510013515e-05,
      "loss": 2.7362,
      "step": 3750
    },
    {
      "epoch": 1.4006634721710283,
      "grad_norm": 0.6910598874092102,
      "learning_rate": 2.6661752057992383e-05,
      "loss": 2.7117,
      "step": 3800
    },
    {
      "epoch": 1.4190932546995945,
      "grad_norm": 0.6466480493545532,
      "learning_rate": 2.6354589015849613e-05,
      "loss": 2.7162,
      "step": 3850
    },
    {
      "epoch": 1.4375230372281607,
      "grad_norm": 0.8473436236381531,
      "learning_rate": 2.6047425973706846e-05,
      "loss": 2.7135,
      "step": 3900
    },
    {
      "epoch": 1.4559528197567269,
      "grad_norm": 0.651626467704773,
      "learning_rate": 2.5740262931564075e-05,
      "loss": 2.7049,
      "step": 3950
    },
    {
      "epoch": 1.474382602285293,
      "grad_norm": 0.6237148642539978,
      "learning_rate": 2.5433099889421308e-05,
      "loss": 2.6918,
      "step": 4000
    },
    {
      "epoch": 1.4928123848138592,
      "grad_norm": 0.5893868207931519,
      "learning_rate": 2.5125936847278537e-05,
      "loss": 2.664,
      "step": 4050
    },
    {
      "epoch": 1.5112421673424254,
      "grad_norm": 0.6019979119300842,
      "learning_rate": 2.4818773805135767e-05,
      "loss": 2.6739,
      "step": 4100
    },
    {
      "epoch": 1.5296719498709916,
      "grad_norm": 1.117405891418457,
      "learning_rate": 2.4511610762993e-05,
      "loss": 2.7494,
      "step": 4150
    },
    {
      "epoch": 1.5481017323995578,
      "grad_norm": 0.9374906420707703,
      "learning_rate": 2.420444772085023e-05,
      "loss": 2.6863,
      "step": 4200
    },
    {
      "epoch": 1.5665315149281238,
      "grad_norm": 0.6033145785331726,
      "learning_rate": 2.389728467870746e-05,
      "loss": 2.6458,
      "step": 4250
    },
    {
      "epoch": 1.58496129745669,
      "grad_norm": 0.8746817111968994,
      "learning_rate": 2.359012163656469e-05,
      "loss": 2.5676,
      "step": 4300
    },
    {
      "epoch": 1.6033910799852562,
      "grad_norm": 0.7059453129768372,
      "learning_rate": 2.328295859442192e-05,
      "loss": 2.7219,
      "step": 4350
    },
    {
      "epoch": 1.6218208625138222,
      "grad_norm": 0.5910465717315674,
      "learning_rate": 2.297579555227915e-05,
      "loss": 2.747,
      "step": 4400
    },
    {
      "epoch": 1.6402506450423884,
      "grad_norm": 1.2678688764572144,
      "learning_rate": 2.266863251013638e-05,
      "loss": 2.6574,
      "step": 4450
    },
    {
      "epoch": 1.6586804275709546,
      "grad_norm": 0.6342867016792297,
      "learning_rate": 2.2361469467993612e-05,
      "loss": 2.7186,
      "step": 4500
    },
    {
      "epoch": 1.6771102100995208,
      "grad_norm": 0.6178333163261414,
      "learning_rate": 2.2054306425850842e-05,
      "loss": 2.7845,
      "step": 4550
    },
    {
      "epoch": 1.695539992628087,
      "grad_norm": 0.5162957906723022,
      "learning_rate": 2.1747143383708075e-05,
      "loss": 2.6223,
      "step": 4600
    },
    {
      "epoch": 1.7139697751566532,
      "grad_norm": 0.6402603387832642,
      "learning_rate": 2.1439980341565304e-05,
      "loss": 2.7258,
      "step": 4650
    },
    {
      "epoch": 1.7323995576852194,
      "grad_norm": 0.7187217473983765,
      "learning_rate": 2.1132817299422534e-05,
      "loss": 2.6987,
      "step": 4700
    },
    {
      "epoch": 1.7508293402137856,
      "grad_norm": 1.2513902187347412,
      "learning_rate": 2.0825654257279763e-05,
      "loss": 2.7838,
      "step": 4750
    },
    {
      "epoch": 1.7692591227423518,
      "grad_norm": 0.7085756063461304,
      "learning_rate": 2.0518491215136996e-05,
      "loss": 2.7708,
      "step": 4800
    },
    {
      "epoch": 1.787688905270918,
      "grad_norm": 0.6793224215507507,
      "learning_rate": 2.0211328172994225e-05,
      "loss": 2.5689,
      "step": 4850
    },
    {
      "epoch": 1.806118687799484,
      "grad_norm": 0.5248484015464783,
      "learning_rate": 1.9904165130851458e-05,
      "loss": 2.6887,
      "step": 4900
    },
    {
      "epoch": 1.8245484703280501,
      "grad_norm": 1.109702706336975,
      "learning_rate": 1.9597002088708688e-05,
      "loss": 2.7141,
      "step": 4950
    },
    {
      "epoch": 1.8429782528566163,
      "grad_norm": 0.6988998055458069,
      "learning_rate": 1.928983904656592e-05,
      "loss": 2.6735,
      "step": 5000
    },
    {
      "epoch": 1.8614080353851823,
      "grad_norm": 0.5965446829795837,
      "learning_rate": 1.898267600442315e-05,
      "loss": 2.6986,
      "step": 5050
    },
    {
      "epoch": 1.8798378179137485,
      "grad_norm": 0.5947949290275574,
      "learning_rate": 1.867551296228038e-05,
      "loss": 2.6331,
      "step": 5100
    },
    {
      "epoch": 1.8982676004423147,
      "grad_norm": 0.8660016655921936,
      "learning_rate": 1.836834992013761e-05,
      "loss": 2.6255,
      "step": 5150
    },
    {
      "epoch": 1.9166973829708809,
      "grad_norm": 0.5962575674057007,
      "learning_rate": 1.806118687799484e-05,
      "loss": 2.6699,
      "step": 5200
    },
    {
      "epoch": 1.935127165499447,
      "grad_norm": 0.6765357851982117,
      "learning_rate": 1.775402383585207e-05,
      "loss": 2.6079,
      "step": 5250
    },
    {
      "epoch": 1.9535569480280133,
      "grad_norm": 0.5553106069564819,
      "learning_rate": 1.74468607937093e-05,
      "loss": 2.6632,
      "step": 5300
    },
    {
      "epoch": 1.9719867305565795,
      "grad_norm": 0.6742434501647949,
      "learning_rate": 1.7139697751566533e-05,
      "loss": 2.6154,
      "step": 5350
    },
    {
      "epoch": 1.9904165130851457,
      "grad_norm": 0.5798575282096863,
      "learning_rate": 1.6832534709423763e-05,
      "loss": 2.6092,
      "step": 5400
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.439124822616577,
      "eval_runtime": 108.6913,
      "eval_samples_per_second": 22.191,
      "eval_steps_per_second": 2.779,
      "step": 5426
    },
    {
      "epoch": 2.008846295613712,
      "grad_norm": 0.6426630616188049,
      "learning_rate": 1.6525371667280992e-05,
      "loss": 2.5909,
      "step": 5450
    },
    {
      "epoch": 2.027276078142278,
      "grad_norm": 0.7080714106559753,
      "learning_rate": 1.621820862513822e-05,
      "loss": 2.6452,
      "step": 5500
    },
    {
      "epoch": 2.0457058606708443,
      "grad_norm": 0.5621817708015442,
      "learning_rate": 1.5911045582995454e-05,
      "loss": 2.6731,
      "step": 5550
    },
    {
      "epoch": 2.0641356431994105,
      "grad_norm": 0.6428905129432678,
      "learning_rate": 1.5603882540852684e-05,
      "loss": 2.6512,
      "step": 5600
    },
    {
      "epoch": 2.082565425727976,
      "grad_norm": 0.7343936562538147,
      "learning_rate": 1.5296719498709917e-05,
      "loss": 2.6404,
      "step": 5650
    },
    {
      "epoch": 2.1009952082565424,
      "grad_norm": 0.6686733961105347,
      "learning_rate": 1.4989556456567146e-05,
      "loss": 2.7008,
      "step": 5700
    },
    {
      "epoch": 2.1194249907851086,
      "grad_norm": 0.5490153431892395,
      "learning_rate": 1.4682393414424377e-05,
      "loss": 2.5706,
      "step": 5750
    },
    {
      "epoch": 2.137854773313675,
      "grad_norm": 0.910538375377655,
      "learning_rate": 1.4375230372281608e-05,
      "loss": 2.6434,
      "step": 5800
    },
    {
      "epoch": 2.156284555842241,
      "grad_norm": 0.5123067498207092,
      "learning_rate": 1.4068067330138838e-05,
      "loss": 2.6165,
      "step": 5850
    },
    {
      "epoch": 2.174714338370807,
      "grad_norm": 0.6437025666236877,
      "learning_rate": 1.3760904287996067e-05,
      "loss": 2.6864,
      "step": 5900
    },
    {
      "epoch": 2.1931441208993734,
      "grad_norm": 0.6816677451133728,
      "learning_rate": 1.3453741245853298e-05,
      "loss": 2.6268,
      "step": 5950
    },
    {
      "epoch": 2.2115739034279396,
      "grad_norm": 0.5700133442878723,
      "learning_rate": 1.314657820371053e-05,
      "loss": 2.6738,
      "step": 6000
    },
    {
      "epoch": 2.2300036859565058,
      "grad_norm": 0.5776288509368896,
      "learning_rate": 1.283941516156776e-05,
      "loss": 2.5453,
      "step": 6050
    },
    {
      "epoch": 2.248433468485072,
      "grad_norm": 0.7419503927230835,
      "learning_rate": 1.2532252119424992e-05,
      "loss": 2.6485,
      "step": 6100
    },
    {
      "epoch": 2.266863251013638,
      "grad_norm": 0.6077136397361755,
      "learning_rate": 1.2225089077282221e-05,
      "loss": 2.6246,
      "step": 6150
    },
    {
      "epoch": 2.2852930335422044,
      "grad_norm": 0.549271285533905,
      "learning_rate": 1.1917926035139452e-05,
      "loss": 2.571,
      "step": 6200
    },
    {
      "epoch": 2.3037228160707706,
      "grad_norm": 0.6384329199790955,
      "learning_rate": 1.1610762992996684e-05,
      "loss": 2.6509,
      "step": 6250
    },
    {
      "epoch": 2.3221525985993363,
      "grad_norm": 0.4970960021018982,
      "learning_rate": 1.1303599950853915e-05,
      "loss": 2.661,
      "step": 6300
    },
    {
      "epoch": 2.3405823811279025,
      "grad_norm": 0.5525777339935303,
      "learning_rate": 1.0996436908711144e-05,
      "loss": 2.5394,
      "step": 6350
    },
    {
      "epoch": 2.3590121636564687,
      "grad_norm": 0.8113371133804321,
      "learning_rate": 1.0689273866568375e-05,
      "loss": 2.562,
      "step": 6400
    },
    {
      "epoch": 2.377441946185035,
      "grad_norm": 0.607928991317749,
      "learning_rate": 1.0382110824425606e-05,
      "loss": 2.6935,
      "step": 6450
    },
    {
      "epoch": 2.395871728713601,
      "grad_norm": 0.4895518720149994,
      "learning_rate": 1.0074947782282836e-05,
      "loss": 2.6191,
      "step": 6500
    },
    {
      "epoch": 2.4143015112421673,
      "grad_norm": 0.7550942301750183,
      "learning_rate": 9.767784740140067e-06,
      "loss": 2.6395,
      "step": 6550
    },
    {
      "epoch": 2.4327312937707335,
      "grad_norm": 0.6614476442337036,
      "learning_rate": 9.460621697997298e-06,
      "loss": 2.6661,
      "step": 6600
    },
    {
      "epoch": 2.4511610762992997,
      "grad_norm": 0.6392025947570801,
      "learning_rate": 9.153458655854528e-06,
      "loss": 2.696,
      "step": 6650
    },
    {
      "epoch": 2.469590858827866,
      "grad_norm": 0.537783682346344,
      "learning_rate": 8.846295613711759e-06,
      "loss": 2.5669,
      "step": 6700
    },
    {
      "epoch": 2.488020641356432,
      "grad_norm": 0.6478318572044373,
      "learning_rate": 8.539132571568988e-06,
      "loss": 2.6122,
      "step": 6750
    },
    {
      "epoch": 2.5064504238849983,
      "grad_norm": 0.7271323204040527,
      "learning_rate": 8.23196952942622e-06,
      "loss": 2.6393,
      "step": 6800
    },
    {
      "epoch": 2.5248802064135645,
      "grad_norm": 0.5243363976478577,
      "learning_rate": 7.92480648728345e-06,
      "loss": 2.674,
      "step": 6850
    },
    {
      "epoch": 2.5433099889421307,
      "grad_norm": 0.5700548887252808,
      "learning_rate": 7.617643445140681e-06,
      "loss": 2.5465,
      "step": 6900
    },
    {
      "epoch": 2.5617397714706964,
      "grad_norm": 0.5878688097000122,
      "learning_rate": 7.310480402997912e-06,
      "loss": 2.6617,
      "step": 6950
    },
    {
      "epoch": 2.580169553999263,
      "grad_norm": 0.6548243761062622,
      "learning_rate": 7.003317360855142e-06,
      "loss": 2.6353,
      "step": 7000
    },
    {
      "epoch": 2.598599336527829,
      "grad_norm": 0.5026042461395264,
      "learning_rate": 6.696154318712373e-06,
      "loss": 2.7388,
      "step": 7050
    },
    {
      "epoch": 2.617029119056395,
      "grad_norm": 0.6485276222229004,
      "learning_rate": 6.388991276569603e-06,
      "loss": 2.6324,
      "step": 7100
    },
    {
      "epoch": 2.635458901584961,
      "grad_norm": 0.5679892897605896,
      "learning_rate": 6.081828234426834e-06,
      "loss": 2.6276,
      "step": 7150
    },
    {
      "epoch": 2.6538886841135274,
      "grad_norm": 0.5411648154258728,
      "learning_rate": 5.774665192284065e-06,
      "loss": 2.5188,
      "step": 7200
    },
    {
      "epoch": 2.6723184666420936,
      "grad_norm": 0.6527003049850464,
      "learning_rate": 5.467502150141295e-06,
      "loss": 2.6614,
      "step": 7250
    },
    {
      "epoch": 2.69074824917066,
      "grad_norm": 0.6237468123435974,
      "learning_rate": 5.160339107998526e-06,
      "loss": 2.6211,
      "step": 7300
    },
    {
      "epoch": 2.709178031699226,
      "grad_norm": 0.7548453211784363,
      "learning_rate": 4.853176065855757e-06,
      "loss": 2.5653,
      "step": 7350
    },
    {
      "epoch": 2.727607814227792,
      "grad_norm": 0.5726668238639832,
      "learning_rate": 4.546013023712987e-06,
      "loss": 2.713,
      "step": 7400
    },
    {
      "epoch": 2.7460375967563584,
      "grad_norm": 0.5756816267967224,
      "learning_rate": 4.238849981570217e-06,
      "loss": 2.6264,
      "step": 7450
    },
    {
      "epoch": 2.7644673792849246,
      "grad_norm": 1.3593347072601318,
      "learning_rate": 3.931686939427448e-06,
      "loss": 2.713,
      "step": 7500
    },
    {
      "epoch": 2.782897161813491,
      "grad_norm": 0.7538343667984009,
      "learning_rate": 3.624523897284679e-06,
      "loss": 2.6891,
      "step": 7550
    },
    {
      "epoch": 2.8013269443420565,
      "grad_norm": 0.6701815724372864,
      "learning_rate": 3.317360855141909e-06,
      "loss": 2.6487,
      "step": 7600
    },
    {
      "epoch": 2.819756726870623,
      "grad_norm": 0.639237105846405,
      "learning_rate": 3.0101978129991403e-06,
      "loss": 2.71,
      "step": 7650
    },
    {
      "epoch": 2.838186509399189,
      "grad_norm": 0.5574836134910583,
      "learning_rate": 2.7030347708563706e-06,
      "loss": 2.5718,
      "step": 7700
    },
    {
      "epoch": 2.856616291927755,
      "grad_norm": 0.6952642798423767,
      "learning_rate": 2.3958717287136013e-06,
      "loss": 2.5824,
      "step": 7750
    },
    {
      "epoch": 2.8750460744563213,
      "grad_norm": 0.8152664303779602,
      "learning_rate": 2.0887086865708316e-06,
      "loss": 2.7234,
      "step": 7800
    },
    {
      "epoch": 2.8934758569848875,
      "grad_norm": 0.6811583638191223,
      "learning_rate": 1.7815456444280625e-06,
      "loss": 2.6181,
      "step": 7850
    },
    {
      "epoch": 2.9119056395134537,
      "grad_norm": 0.676243245601654,
      "learning_rate": 1.4743826022852932e-06,
      "loss": 2.619,
      "step": 7900
    },
    {
      "epoch": 2.93033542204202,
      "grad_norm": 0.6834225058555603,
      "learning_rate": 1.1672195601425237e-06,
      "loss": 2.5703,
      "step": 7950
    },
    {
      "epoch": 2.948765204570586,
      "grad_norm": 0.61197829246521,
      "learning_rate": 8.600565179997542e-07,
      "loss": 2.6257,
      "step": 8000
    },
    {
      "epoch": 2.9671949870991523,
      "grad_norm": 0.6163896322250366,
      "learning_rate": 5.528934758569849e-07,
      "loss": 2.5942,
      "step": 8050
    },
    {
      "epoch": 2.9856247696277185,
      "grad_norm": 0.5979213714599609,
      "learning_rate": 2.457304337142155e-07,
      "loss": 2.5678,
      "step": 8100
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.4096922874450684,
      "eval_runtime": 109.412,
      "eval_samples_per_second": 22.045,
      "eval_steps_per_second": 2.76,
      "step": 8139
    }
  ],
  "logging_steps": 50,
  "max_steps": 8139,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.76215025516544e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
