{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 5426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018429782528566162,
      "grad_norm": 1.2773523330688477,
      "learning_rate": 4.9698980218700083e-05,
      "loss": 5.4159,
      "step": 50
    },
    {
      "epoch": 0.036859565057132324,
      "grad_norm": 0.773535430431366,
      "learning_rate": 4.939181717655732e-05,
      "loss": 3.6389,
      "step": 100
    },
    {
      "epoch": 0.055289347585698485,
      "grad_norm": 0.7981192469596863,
      "learning_rate": 4.908465413441455e-05,
      "loss": 3.4313,
      "step": 150
    },
    {
      "epoch": 0.07371913011426465,
      "grad_norm": 0.6978963017463684,
      "learning_rate": 4.8777491092271785e-05,
      "loss": 3.3298,
      "step": 200
    },
    {
      "epoch": 0.09214891264283082,
      "grad_norm": 0.7759546637535095,
      "learning_rate": 4.847032805012901e-05,
      "loss": 3.3619,
      "step": 250
    },
    {
      "epoch": 0.11057869517139697,
      "grad_norm": 0.7568705677986145,
      "learning_rate": 4.8163165007986244e-05,
      "loss": 3.2831,
      "step": 300
    },
    {
      "epoch": 0.12900847769996315,
      "grad_norm": 0.6799579858779907,
      "learning_rate": 4.7856001965843474e-05,
      "loss": 3.2067,
      "step": 350
    },
    {
      "epoch": 0.1474382602285293,
      "grad_norm": 0.5349483489990234,
      "learning_rate": 4.75488389237007e-05,
      "loss": 3.153,
      "step": 400
    },
    {
      "epoch": 0.16586804275709546,
      "grad_norm": 0.7624514102935791,
      "learning_rate": 4.724167588155793e-05,
      "loss": 3.1923,
      "step": 450
    },
    {
      "epoch": 0.18429782528566163,
      "grad_norm": 0.8336101770401001,
      "learning_rate": 4.693451283941516e-05,
      "loss": 3.1732,
      "step": 500
    },
    {
      "epoch": 0.2027276078142278,
      "grad_norm": 0.9387838244438171,
      "learning_rate": 4.66273497972724e-05,
      "loss": 3.178,
      "step": 550
    },
    {
      "epoch": 0.22115739034279394,
      "grad_norm": 0.909470796585083,
      "learning_rate": 4.632018675512962e-05,
      "loss": 3.1487,
      "step": 600
    },
    {
      "epoch": 0.2395871728713601,
      "grad_norm": 0.8461583256721497,
      "learning_rate": 4.601302371298686e-05,
      "loss": 3.1323,
      "step": 650
    },
    {
      "epoch": 0.2580169553999263,
      "grad_norm": 0.7184539437294006,
      "learning_rate": 4.5705860670844087e-05,
      "loss": 3.1017,
      "step": 700
    },
    {
      "epoch": 0.27644673792849245,
      "grad_norm": 0.7614105939865112,
      "learning_rate": 4.5398697628701316e-05,
      "loss": 3.2316,
      "step": 750
    },
    {
      "epoch": 0.2948765204570586,
      "grad_norm": 0.6786227822303772,
      "learning_rate": 4.5091534586558545e-05,
      "loss": 3.0008,
      "step": 800
    },
    {
      "epoch": 0.3133063029856248,
      "grad_norm": 0.6684582233428955,
      "learning_rate": 4.4784371544415775e-05,
      "loss": 3.1015,
      "step": 850
    },
    {
      "epoch": 0.3317360855141909,
      "grad_norm": 0.7357620596885681,
      "learning_rate": 4.447720850227301e-05,
      "loss": 2.9619,
      "step": 900
    },
    {
      "epoch": 0.3501658680427571,
      "grad_norm": 0.8081740736961365,
      "learning_rate": 4.417004546013024e-05,
      "loss": 2.9309,
      "step": 950
    },
    {
      "epoch": 0.36859565057132326,
      "grad_norm": 0.6598358750343323,
      "learning_rate": 4.386288241798747e-05,
      "loss": 3.0565,
      "step": 1000
    },
    {
      "epoch": 0.3870254330998894,
      "grad_norm": 0.7431583404541016,
      "learning_rate": 4.35557193758447e-05,
      "loss": 3.0639,
      "step": 1050
    },
    {
      "epoch": 0.4054552156284556,
      "grad_norm": 12.191261291503906,
      "learning_rate": 4.324855633370193e-05,
      "loss": 2.9345,
      "step": 1100
    },
    {
      "epoch": 0.42388499815702174,
      "grad_norm": 0.642913281917572,
      "learning_rate": 4.2941393291559165e-05,
      "loss": 2.9943,
      "step": 1150
    },
    {
      "epoch": 0.4423147806855879,
      "grad_norm": 0.6810293793678284,
      "learning_rate": 4.263423024941639e-05,
      "loss": 2.9873,
      "step": 1200
    },
    {
      "epoch": 0.4607445632141541,
      "grad_norm": 0.559029221534729,
      "learning_rate": 4.2327067207273624e-05,
      "loss": 3.0121,
      "step": 1250
    },
    {
      "epoch": 0.4791743457427202,
      "grad_norm": 0.7281472086906433,
      "learning_rate": 4.2019904165130853e-05,
      "loss": 2.9706,
      "step": 1300
    },
    {
      "epoch": 0.4976041282712864,
      "grad_norm": 0.8445867896080017,
      "learning_rate": 4.171274112298808e-05,
      "loss": 2.9015,
      "step": 1350
    },
    {
      "epoch": 0.5160339107998526,
      "grad_norm": 0.5654202699661255,
      "learning_rate": 4.140557808084531e-05,
      "loss": 2.9434,
      "step": 1400
    },
    {
      "epoch": 0.5344636933284187,
      "grad_norm": 0.6119396686553955,
      "learning_rate": 4.109841503870255e-05,
      "loss": 2.8656,
      "step": 1450
    },
    {
      "epoch": 0.5528934758569849,
      "grad_norm": 0.7492556571960449,
      "learning_rate": 4.079125199655978e-05,
      "loss": 2.8317,
      "step": 1500
    },
    {
      "epoch": 0.5713232583855511,
      "grad_norm": 0.5814321637153625,
      "learning_rate": 4.048408895441701e-05,
      "loss": 2.9019,
      "step": 1550
    },
    {
      "epoch": 0.5897530409141172,
      "grad_norm": 0.6305776238441467,
      "learning_rate": 4.017692591227424e-05,
      "loss": 2.8512,
      "step": 1600
    },
    {
      "epoch": 0.6081828234426834,
      "grad_norm": 0.6496685743331909,
      "learning_rate": 3.9869762870131466e-05,
      "loss": 2.8306,
      "step": 1650
    },
    {
      "epoch": 0.6266126059712496,
      "grad_norm": 0.5742912888526917,
      "learning_rate": 3.95625998279887e-05,
      "loss": 2.8608,
      "step": 1700
    },
    {
      "epoch": 0.6450423884998157,
      "grad_norm": 0.5767165422439575,
      "learning_rate": 3.9255436785845925e-05,
      "loss": 2.8948,
      "step": 1750
    },
    {
      "epoch": 0.6634721710283819,
      "grad_norm": 0.6612684726715088,
      "learning_rate": 3.894827374370316e-05,
      "loss": 2.8112,
      "step": 1800
    },
    {
      "epoch": 0.681901953556948,
      "grad_norm": 0.5990036725997925,
      "learning_rate": 3.864111070156039e-05,
      "loss": 2.8635,
      "step": 1850
    },
    {
      "epoch": 0.7003317360855142,
      "grad_norm": 0.9205067157745361,
      "learning_rate": 3.833394765941762e-05,
      "loss": 2.8299,
      "step": 1900
    },
    {
      "epoch": 0.7187615186140803,
      "grad_norm": 0.7069308757781982,
      "learning_rate": 3.802678461727485e-05,
      "loss": 2.8889,
      "step": 1950
    },
    {
      "epoch": 0.7371913011426465,
      "grad_norm": 0.5094358921051025,
      "learning_rate": 3.771962157513208e-05,
      "loss": 2.8024,
      "step": 2000
    },
    {
      "epoch": 0.7556210836712127,
      "grad_norm": 0.6244612336158752,
      "learning_rate": 3.7412458532989315e-05,
      "loss": 2.8496,
      "step": 2050
    },
    {
      "epoch": 0.7740508661997788,
      "grad_norm": 0.6748182773590088,
      "learning_rate": 3.7105295490846545e-05,
      "loss": 2.8246,
      "step": 2100
    },
    {
      "epoch": 0.792480648728345,
      "grad_norm": 0.7174170017242432,
      "learning_rate": 3.6798132448703774e-05,
      "loss": 2.826,
      "step": 2150
    },
    {
      "epoch": 0.8109104312569112,
      "grad_norm": 0.523895800113678,
      "learning_rate": 3.6490969406561004e-05,
      "loss": 2.7808,
      "step": 2200
    },
    {
      "epoch": 0.8293402137854773,
      "grad_norm": 1.0455775260925293,
      "learning_rate": 3.618380636441823e-05,
      "loss": 2.8807,
      "step": 2250
    },
    {
      "epoch": 0.8477699963140435,
      "grad_norm": 0.6181491613388062,
      "learning_rate": 3.587664332227547e-05,
      "loss": 2.8208,
      "step": 2300
    },
    {
      "epoch": 0.8661997788426097,
      "grad_norm": 0.5946605205535889,
      "learning_rate": 3.556948028013269e-05,
      "loss": 2.8167,
      "step": 2350
    },
    {
      "epoch": 0.8846295613711758,
      "grad_norm": 0.5858719944953918,
      "learning_rate": 3.526231723798993e-05,
      "loss": 2.8049,
      "step": 2400
    },
    {
      "epoch": 0.903059343899742,
      "grad_norm": 0.6393921971321106,
      "learning_rate": 3.495515419584716e-05,
      "loss": 2.7678,
      "step": 2450
    },
    {
      "epoch": 0.9214891264283082,
      "grad_norm": 0.503508985042572,
      "learning_rate": 3.464799115370439e-05,
      "loss": 2.8983,
      "step": 2500
    },
    {
      "epoch": 0.9399189089568744,
      "grad_norm": 0.6633691787719727,
      "learning_rate": 3.434082811156162e-05,
      "loss": 2.7179,
      "step": 2550
    },
    {
      "epoch": 0.9583486914854404,
      "grad_norm": 0.7118969559669495,
      "learning_rate": 3.403366506941885e-05,
      "loss": 2.7696,
      "step": 2600
    },
    {
      "epoch": 0.9767784740140066,
      "grad_norm": 0.5705476403236389,
      "learning_rate": 3.372650202727608e-05,
      "loss": 2.8249,
      "step": 2650
    },
    {
      "epoch": 0.9952082565425728,
      "grad_norm": 0.6078850030899048,
      "learning_rate": 3.3419338985133305e-05,
      "loss": 2.7878,
      "step": 2700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.55727481842041,
      "eval_runtime": 105.757,
      "eval_samples_per_second": 22.807,
      "eval_steps_per_second": 2.856,
      "step": 2713
    },
    {
      "epoch": 1.013638039071139,
      "grad_norm": 0.6188091039657593,
      "learning_rate": 3.311217594299054e-05,
      "loss": 2.8671,
      "step": 2750
    },
    {
      "epoch": 1.0320678215997052,
      "grad_norm": 0.7471727728843689,
      "learning_rate": 3.280501290084777e-05,
      "loss": 2.8682,
      "step": 2800
    },
    {
      "epoch": 1.0504976041282712,
      "grad_norm": 0.5536347031593323,
      "learning_rate": 3.249784985870501e-05,
      "loss": 2.799,
      "step": 2850
    },
    {
      "epoch": 1.0689273866568374,
      "grad_norm": 0.5562173128128052,
      "learning_rate": 3.219068681656223e-05,
      "loss": 2.7356,
      "step": 2900
    },
    {
      "epoch": 1.0873571691854036,
      "grad_norm": 0.6290777325630188,
      "learning_rate": 3.1883523774419466e-05,
      "loss": 2.6605,
      "step": 2950
    },
    {
      "epoch": 1.1057869517139698,
      "grad_norm": 0.6797436475753784,
      "learning_rate": 3.1576360732276695e-05,
      "loss": 2.8338,
      "step": 3000
    },
    {
      "epoch": 1.124216734242536,
      "grad_norm": 0.7407424449920654,
      "learning_rate": 3.1269197690133925e-05,
      "loss": 2.711,
      "step": 3050
    },
    {
      "epoch": 1.1426465167711022,
      "grad_norm": 0.9832471609115601,
      "learning_rate": 3.0962034647991154e-05,
      "loss": 2.8085,
      "step": 3100
    },
    {
      "epoch": 1.1610762992996682,
      "grad_norm": 0.6796048879623413,
      "learning_rate": 3.0654871605848384e-05,
      "loss": 2.7101,
      "step": 3150
    },
    {
      "epoch": 1.1795060818282344,
      "grad_norm": 0.6645605564117432,
      "learning_rate": 3.0347708563705616e-05,
      "loss": 2.7523,
      "step": 3200
    },
    {
      "epoch": 1.1979358643568006,
      "grad_norm": 0.6157718896865845,
      "learning_rate": 3.0040545521562846e-05,
      "loss": 2.7405,
      "step": 3250
    },
    {
      "epoch": 1.2163656468853667,
      "grad_norm": 1.0044106245040894,
      "learning_rate": 2.973338247942008e-05,
      "loss": 2.7451,
      "step": 3300
    },
    {
      "epoch": 1.234795429413933,
      "grad_norm": 0.5953869819641113,
      "learning_rate": 2.9426219437277308e-05,
      "loss": 2.7718,
      "step": 3350
    },
    {
      "epoch": 1.2532252119424991,
      "grad_norm": 0.6016568541526794,
      "learning_rate": 2.9119056395134538e-05,
      "loss": 2.6943,
      "step": 3400
    },
    {
      "epoch": 1.2716549944710653,
      "grad_norm": 0.5510916113853455,
      "learning_rate": 2.881189335299177e-05,
      "loss": 2.6673,
      "step": 3450
    },
    {
      "epoch": 1.2900847769996315,
      "grad_norm": 0.5760928392410278,
      "learning_rate": 2.8504730310849e-05,
      "loss": 2.75,
      "step": 3500
    },
    {
      "epoch": 1.3085145595281975,
      "grad_norm": 0.6176618337631226,
      "learning_rate": 2.8197567268706233e-05,
      "loss": 2.7359,
      "step": 3550
    },
    {
      "epoch": 1.3269443420567637,
      "grad_norm": 0.5918596982955933,
      "learning_rate": 2.789040422656346e-05,
      "loss": 2.6959,
      "step": 3600
    },
    {
      "epoch": 1.34537412458533,
      "grad_norm": 0.9635293483734131,
      "learning_rate": 2.7583241184420695e-05,
      "loss": 2.5851,
      "step": 3650
    },
    {
      "epoch": 1.363803907113896,
      "grad_norm": 0.6354343891143799,
      "learning_rate": 2.727607814227792e-05,
      "loss": 2.6662,
      "step": 3700
    },
    {
      "epoch": 1.3822336896424623,
      "grad_norm": 0.5506047606468201,
      "learning_rate": 2.696891510013515e-05,
      "loss": 2.7362,
      "step": 3750
    },
    {
      "epoch": 1.4006634721710283,
      "grad_norm": 0.6910598874092102,
      "learning_rate": 2.6661752057992383e-05,
      "loss": 2.7117,
      "step": 3800
    },
    {
      "epoch": 1.4190932546995945,
      "grad_norm": 0.6466480493545532,
      "learning_rate": 2.6354589015849613e-05,
      "loss": 2.7162,
      "step": 3850
    },
    {
      "epoch": 1.4375230372281607,
      "grad_norm": 0.8473436236381531,
      "learning_rate": 2.6047425973706846e-05,
      "loss": 2.7135,
      "step": 3900
    },
    {
      "epoch": 1.4559528197567269,
      "grad_norm": 0.651626467704773,
      "learning_rate": 2.5740262931564075e-05,
      "loss": 2.7049,
      "step": 3950
    },
    {
      "epoch": 1.474382602285293,
      "grad_norm": 0.6237148642539978,
      "learning_rate": 2.5433099889421308e-05,
      "loss": 2.6918,
      "step": 4000
    },
    {
      "epoch": 1.4928123848138592,
      "grad_norm": 0.5893868207931519,
      "learning_rate": 2.5125936847278537e-05,
      "loss": 2.664,
      "step": 4050
    },
    {
      "epoch": 1.5112421673424254,
      "grad_norm": 0.6019979119300842,
      "learning_rate": 2.4818773805135767e-05,
      "loss": 2.6739,
      "step": 4100
    },
    {
      "epoch": 1.5296719498709916,
      "grad_norm": 1.117405891418457,
      "learning_rate": 2.4511610762993e-05,
      "loss": 2.7494,
      "step": 4150
    },
    {
      "epoch": 1.5481017323995578,
      "grad_norm": 0.9374906420707703,
      "learning_rate": 2.420444772085023e-05,
      "loss": 2.6863,
      "step": 4200
    },
    {
      "epoch": 1.5665315149281238,
      "grad_norm": 0.6033145785331726,
      "learning_rate": 2.389728467870746e-05,
      "loss": 2.6458,
      "step": 4250
    },
    {
      "epoch": 1.58496129745669,
      "grad_norm": 0.8746817111968994,
      "learning_rate": 2.359012163656469e-05,
      "loss": 2.5676,
      "step": 4300
    },
    {
      "epoch": 1.6033910799852562,
      "grad_norm": 0.7059453129768372,
      "learning_rate": 2.328295859442192e-05,
      "loss": 2.7219,
      "step": 4350
    },
    {
      "epoch": 1.6218208625138222,
      "grad_norm": 0.5910465717315674,
      "learning_rate": 2.297579555227915e-05,
      "loss": 2.747,
      "step": 4400
    },
    {
      "epoch": 1.6402506450423884,
      "grad_norm": 1.2678688764572144,
      "learning_rate": 2.266863251013638e-05,
      "loss": 2.6574,
      "step": 4450
    },
    {
      "epoch": 1.6586804275709546,
      "grad_norm": 0.6342867016792297,
      "learning_rate": 2.2361469467993612e-05,
      "loss": 2.7186,
      "step": 4500
    },
    {
      "epoch": 1.6771102100995208,
      "grad_norm": 0.6178333163261414,
      "learning_rate": 2.2054306425850842e-05,
      "loss": 2.7845,
      "step": 4550
    },
    {
      "epoch": 1.695539992628087,
      "grad_norm": 0.5162957906723022,
      "learning_rate": 2.1747143383708075e-05,
      "loss": 2.6223,
      "step": 4600
    },
    {
      "epoch": 1.7139697751566532,
      "grad_norm": 0.6402603387832642,
      "learning_rate": 2.1439980341565304e-05,
      "loss": 2.7258,
      "step": 4650
    },
    {
      "epoch": 1.7323995576852194,
      "grad_norm": 0.7187217473983765,
      "learning_rate": 2.1132817299422534e-05,
      "loss": 2.6987,
      "step": 4700
    },
    {
      "epoch": 1.7508293402137856,
      "grad_norm": 1.2513902187347412,
      "learning_rate": 2.0825654257279763e-05,
      "loss": 2.7838,
      "step": 4750
    },
    {
      "epoch": 1.7692591227423518,
      "grad_norm": 0.7085756063461304,
      "learning_rate": 2.0518491215136996e-05,
      "loss": 2.7708,
      "step": 4800
    },
    {
      "epoch": 1.787688905270918,
      "grad_norm": 0.6793224215507507,
      "learning_rate": 2.0211328172994225e-05,
      "loss": 2.5689,
      "step": 4850
    },
    {
      "epoch": 1.806118687799484,
      "grad_norm": 0.5248484015464783,
      "learning_rate": 1.9904165130851458e-05,
      "loss": 2.6887,
      "step": 4900
    },
    {
      "epoch": 1.8245484703280501,
      "grad_norm": 1.109702706336975,
      "learning_rate": 1.9597002088708688e-05,
      "loss": 2.7141,
      "step": 4950
    },
    {
      "epoch": 1.8429782528566163,
      "grad_norm": 0.6988998055458069,
      "learning_rate": 1.928983904656592e-05,
      "loss": 2.6735,
      "step": 5000
    },
    {
      "epoch": 1.8614080353851823,
      "grad_norm": 0.5965446829795837,
      "learning_rate": 1.898267600442315e-05,
      "loss": 2.6986,
      "step": 5050
    },
    {
      "epoch": 1.8798378179137485,
      "grad_norm": 0.5947949290275574,
      "learning_rate": 1.867551296228038e-05,
      "loss": 2.6331,
      "step": 5100
    },
    {
      "epoch": 1.8982676004423147,
      "grad_norm": 0.8660016655921936,
      "learning_rate": 1.836834992013761e-05,
      "loss": 2.6255,
      "step": 5150
    },
    {
      "epoch": 1.9166973829708809,
      "grad_norm": 0.5962575674057007,
      "learning_rate": 1.806118687799484e-05,
      "loss": 2.6699,
      "step": 5200
    },
    {
      "epoch": 1.935127165499447,
      "grad_norm": 0.6765357851982117,
      "learning_rate": 1.775402383585207e-05,
      "loss": 2.6079,
      "step": 5250
    },
    {
      "epoch": 1.9535569480280133,
      "grad_norm": 0.5553106069564819,
      "learning_rate": 1.74468607937093e-05,
      "loss": 2.6632,
      "step": 5300
    },
    {
      "epoch": 1.9719867305565795,
      "grad_norm": 0.6742434501647949,
      "learning_rate": 1.7139697751566533e-05,
      "loss": 2.6154,
      "step": 5350
    },
    {
      "epoch": 1.9904165130851457,
      "grad_norm": 0.5798575282096863,
      "learning_rate": 1.6832534709423763e-05,
      "loss": 2.6092,
      "step": 5400
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.439124822616577,
      "eval_runtime": 108.6913,
      "eval_samples_per_second": 22.191,
      "eval_steps_per_second": 2.779,
      "step": 5426
    }
  ],
  "logging_steps": 50,
  "max_steps": 8139,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.17476683677696e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
